---
title: "Module 1 â€” LLM Interfaces"
module: "Module 1: Foundations for LLM Builders"
subtopic: "llm-interfaces"
summary: "Work directly with LLM client libraries/runtimes and solid request/response patterns."
status: in-progress
articles:
  - title: "Choosing the Right LLM Client (openai, groq, or litellm?)"
    description: "Compare three client SDKs, request patterns, and ergonomics for structured outputs."
    type: article
    status: planned
---

## Scope
- Build a repeatable local workflow for LLM requests (CLI + scripts).
- Capture JSON I/O for fast diffing between prompt iterations.
- Introduce basic tools (search, calculator) to prep for Module 3 integrations.

## Starter artifacts
- `scripts/client.ts` (or Python equivalent) with env-based key loading and retry/backoff.
- `logs/requests.jsonl` format: `{ts, model, prompt, tools, latency_ms, output, trace_id}`.
- Tool harness: simple search API stub + deterministic calculator function for model testing.

## Quick wins to ship
- Minimal CLI: `pnpm dev:ask "Summarize doc X"` that logs the exchange.
- Example function-call schema for calculator with validation and guardrails.
- Short write-up comparing SDK ergonomics for streaming and JSON output modes.
