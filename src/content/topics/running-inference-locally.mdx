---
title: "Running inference locally"
module: "Module 0: Setup & Safety"
subtopic: "running-inference-locally"
summary: "Easily setup & run an LLM on your own machine."
---

## Why host locally
- No cloud nor hosting cost
- Quick test & iteration
- Privacy conversations: keeps data on-device. It can be exported later
- Ideal for learning the runtime stack before scaling to a hosted model.

## Baseline checklist
1. Pick a model size that fits your CPU/GPU and RAM budget.
2. Install a runtime (Ollama, LM Studio, or llama.cpp).
3. Download weights, verify checksums, and track versions.
4. Start a local server and confirm the health endpoint.
5. Send a quick prompt from the CLI and capture latency.

## Screenshot on how it looks
```bash
# TODO: Add command for llama.cpp CLI
```
