---
title: "Building & publishing an MCP server as an npm package"
module: "Module 2: AI Integration & Orchestration"
subtopic: "building-mcp-server"
summary: "Building a knowledge-base MCP server that exposes published articles and frameworks to any AI client, packaged as an npm install."
status: "planned"
---

## What this is about

I'm building an MCP server. Not as an exercise, as infrastructure. Something people can `npm install` and immediately connect to their AI tools to search and read my published content.

This article walks through the architecture, implementation, and what I learned. Starting simple: a read-only knowledge base. No API keys, no credentials, no premium tiers. Just content.

---

## MCP in 60 seconds

Model Context Protocol is Anthropic's open standard for connecting AI models to external data and tools. Think of it as a USB-C port for AI. A single interface that any compliant client (Claude Desktop, Cursor, Windsurf) can plug into.

An MCP server exposes three primitives:

- **Tools** — functions the model can call
- **Resources** — data the model can read
- **Prompts** — reusable prompt templates

For this first version, I'm only using **Resources**. The server exposes my published articles as readable content. No executable tools, no prompt templates. Just a searchable knowledge base.

The cost model is important: the MCP server is lightweight. The heavy lifting (deciding when to search, processing results) happens on the client's LLM. Users spend their own tokens. I just serve the data.

---

## Version 1: Knowledge base only

The simplest useful thing. Two capabilities:

1. **Search articles** — takes a query, returns matching titles, summaries, and URLs
2. **Get article** — takes a slug, returns the full article text

Content comes from a static JSON index hosted on one of my sites. The MCP server fetches this index on startup and holds it in memory. When the user's AI needs information, it searches the index and retrieves relevant articles.

**What it exposes:**
- All published articles across the Turtleand ecosystem (AI Lab, blog, OpenClaw Lab, Build Log)
- Article metadata: title, summary, module, tags, canonical URL
- Full article text for deep reading

**What it doesn't expose:**
- No executable tools (no formatting, no analysis, no API calls)
- No filesystem access
- No credentials or environment variables required
- No network calls except fetching the public content index

Zero configuration. Install and go.

---

## Architecture

```
@turtleand/mcp-server
├── src/
│   ├── index.ts          # Server entry, stdio transport
│   ├── resources.ts      # Article index + content serving
│   └── types.ts          # Shared types
├── package.json
├── tsconfig.json
└── README.md
```

That's it. Three source files. The index fetches the content catalog on startup. Resources registers two MCP resources: the search endpoint and the article reader.

### Why a separate repo?

Yes, this needs its own repo. Three reasons:

1. **npm publishing.** You publish from a repo's root. Mixing this into an Astro site would mean publishing the entire site as an npm package, or fighting with monorepo tooling. Separate repo means clean `npm publish`.

2. **Independent versioning.** The MCP server has its own release cycle. Article content changes daily. The server code changes rarely. Coupling them creates unnecessary releases.

3. **Clear dependency boundary.** The MCP server depends on `@modelcontextprotocol/sdk` and nothing else. An Astro site has dozens of dependencies. Keeping them separate means a tiny, auditable `node_modules`.

The repo would be something like `github.com/turtleand/mcp-server`.

### Where to host the content index?

The JSON index file needs a public URL. Best candidate: **turtleand.com** (the portal). It's the hub that already links to all other sites. Hosting the index there makes conceptual sense. It's the front door.

The index gets rebuilt by a script that reads all articles across all repos and outputs a single `content-index.json`. This runs on deploy or via cron. The MCP server fetches it from `https://turtleand.com/content-index.json`.

Alternative: bundle the index inside the npm package. Simpler (no network fetch on startup), but means publishing a new package version every time content changes. For v1, bundling might actually be the right call. Update the package weekly or when significant content drops.

---

## Implementation

### The server

```typescript
import { McpServer } from "@modelcontextprotocol/sdk/server/mcp.js";
import { StdioServerTransport } from "@modelcontextprotocol/sdk/server/stdio.js";

const server = new McpServer({
  name: "@turtleand/mcp-server",
  version: "0.1.0",
});
```

### Registering resources

```typescript
import { z } from "zod";
import contentIndex from "./content-index.json";

server.tool(
  "search-articles",
  "Search Turtleand's published articles on AI integration, workflows, and career transition",
  { query: z.string().describe("Search query") },
  async ({ query }) => {
    const results = searchIndex(query, contentIndex);
    return {
      content: [{ type: "text", text: JSON.stringify(results, null, 2) }],
    };
  }
);

server.tool(
  "get-article",
  "Get the full text of a Turtleand article by slug",
  { slug: z.string().describe("Article slug, e.g. 'expand-filter-absorb'") },
  async ({ slug }) => {
    const article = contentIndex.find(a => a.slug === slug);
    return {
      content: [{ type: "text", text: article?.body ?? "Article not found" }],
    };
  }
);
```

The search is simple keyword matching for v1. Title contains query, summary contains query, tags match. With 30 articles, this works fine. Vector embeddings are a future optimization, not a launch requirement.

### Transport

Stdio only. The client spawns the server as a subprocess and communicates via stdin/stdout. Simple, secure, works everywhere.

```typescript
const transport = new StdioServerTransport();
await server.connect(transport);
```

No HTTP server, no SSE, no ports to configure. The user's AI client handles everything.

---

## Installation

Users add one block to their AI client config. For Claude Desktop:

```json
{
  "mcpServers": {
    "turtleand": {
      "command": "npx",
      "args": ["-y", "@turtleand/mcp-server"]
    }
  }
}
```

No API keys. No environment variables. No Docker. One JSON block.

For Cursor, similar config in settings under MCP servers.

---

## Security considerations

An MCP server runs with the permissions of the user who starts it. Publishing one as an npm package means thinking about trust, even for a read-only knowledge base.

**What makes v1 safe:**

1. **No credentials required.** The package ships with zero secrets. No API keys, no environment variables, no authentication.
2. **No filesystem access.** The server never reads or writes files on the user's machine.
3. **No outbound network calls.** Content is bundled in the package. Nothing phones home.
4. **No data collection.** No telemetry, no usage tracking, no analytics.
5. **Minimal dependencies.** Just the MCP SDK. Small surface area, easy to audit.
6. **Open source.** Full source on GitHub. Anyone can read every line before installing.

**Residual risks:**

- **Supply chain.** If someone compromises the npm account, they could push malicious code. Mitigation: 2FA on npm, signed commits, pinned dependencies.
- **Content injection.** Articles fed into the user's AI context could theoretically contain prompt injection. Since I control all the content, this is a non-issue. But it's worth noting for transparency.

---

## What's next

Version 1 ships as a knowledge base. Search and read articles. That's it.

Future versions could add:

- Executable tools (content formatting, framework assessments)
- Dynamic content fetching instead of bundled index
- Prompt templates encoding useful patterns
- Plugin architecture for community contributions

But those are future problems. The first version needs to exist and be useful. Everything else comes from real usage, not speculation.

---

## Current status

**Status: Planning**

Next steps: create the repo, scaffold the package, build the content index generator, publish v0.1.0.
