---
title: "Module 0 - Model access & runtimes"
module: "Module 0: Setup & Safety"
subtopic: "model-access-runtimes"
summary: "Choose hosted APIs vs local runtimes (Ollama, vLLM), set them up, and understand trade-offs."
status: in-progress
articles:
  - title: "Setup & run inference locally"
    description: "Run an LLM on your own machine for fast iteration, offline testing, and tighter data control."
    link: "/topics/module-0-model-access/running-inference-locally/"
  - title: "Setup & run inference on the cloud"
    description: "Host"
    link: "/topics/module-0-model-access/running-inference-cloud/"
---

## Internal notes
- Decide when to use hosted APIs vs local runtimes.
- Compare CPU-only vs GPU-backed setups for small/medium models.
- Track latency, memory, and quality trade-offs across setups.
