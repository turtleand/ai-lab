---
title: "Module 0 - Model access & runtimes"
module: "Module 0: Setup & Safety"
subtopic: "model-access-runtimes"
summary: "Choose hosted APIs vs local runtimes (Ollama, vLLM), set them up, and understand trade-offs."
status: in-progress
articles:
  - title: "Hosting a model locally"
    description: "Run an LLM on your own machine for fast iteration, offline testing, and tighter data control."
    type: article
    status: planned
    link: "/topics/module-0-model-access/hosting-a-model-locally/"
---

## Scope
- Decide when to use hosted APIs vs local runtimes.
- Compare CPU-only vs GPU-backed setups for small/medium models.
- Track latency, memory, and quality trade-offs across setups.

## Starter artifacts
- Local runtime checklist (drivers, model cache, storage budget).
- One script to start/stop the local model server.
- Simple prompt harness to benchmark latency and quality.

## Quick wins to ship
- One-page setup guide with screenshots and troubleshooting tips.
- A short benchmark log for 2-3 model sizes on your machine.
- Notes on when to switch from local to hosted APIs.
