---
title: "Ejecutar inferencia localmente"
module: "Módulo 0: Configuración y seguridad"
subtopic: "running-inference-locally"
summary: "Configura y ejecuta un LLM en tu propia máquina con facilidad."
---

## Por qué ejecutar en local
- Sin costos de nube ni hosting
- Pruebas e iteración rápidas
- Privacidad de las conversaciones: los datos se quedan en el dispositivo. Se pueden exportar después
- Ideal para aprender la pila de ejecución antes de escalar a un modelo alojado.

## Checklist base
1. Elige un tamaño de modelo que se ajuste a tu presupuesto de CPU/GPU y RAM.
2. Instala un runtime (Ollama, LM Studio o llama.cpp).
3. Descarga los pesos, verifica las sumas de verificación y registra versiones.
4. Inicia un servidor local y confirma el endpoint de salud.
5. Envía un prompt rápido desde la CLI y captura la latencia.

## Captura de pantalla de cómo se ve
```bash
# TODO: Agregar comando para la CLI de llama.cpp
```
