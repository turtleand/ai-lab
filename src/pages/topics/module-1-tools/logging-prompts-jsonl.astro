---
import BaseLayout from '../../../layouts/BaseLayout.astro';

const title = 'Logging Prompts and Responses to JSONL';
const description =
  'Implement a tiny logger to capture prompts, outputs, latency, and model params for fast inspection and diffing.';
---

<BaseLayout title={title} description={description}>
  <section class="card" aria-labelledby="article-title">
    <div class="pill inline">Module 1 · LLM Dev Environment & APIs</div>
    <h1 id="article-title">{title}</h1>
    <p>{description}</p>
    <div class="tag-row">
      <span class="status in-progress">in-progress</span>
      <a class="pill" href="/topics/module-1-tools/">Back to Module 1 · Tools</a>
    </div>
  </section>

  <section class="card" aria-labelledby="why-title">
    <div class="section-title">
      <div>
        <p class="pill inline">POC</p>
        <h2 id="why-title">What we want</h2>
      </div>
      <small>Portable logging for every LLM call</small>
    </div>
    <p>
      Capture every request/response into a simple JSONL file to debug regressions, compare prompts,
      and measure latency. The logger must be transport-agnostic so it can wrap OpenAI, Groq, or any
      future client.
    </p>
  </section>

  <section class="card" aria-labelledby="snippet-title">
    <div class="section-title">
      <div>
        <p class="pill inline">Code</p>
        <h2 id="snippet-title">Minimal logger sketch (TypeScript)</h2>
      </div>
      <small>Swap the client to match your SDK</small>
    </div>
    <pre><code>{
`import fs from 'node:fs';
import { randomUUID } from 'node:crypto';

type LogEntry = {
  ts: string;
  trace_id: string;
  model: string;
  prompt: string;
  tools?: string[];
  latency_ms: number;
  output: string;
};

const stream = fs.createWriteStream('logs/requests.jsonl', { flags: 'a' });

export async function askLLM(prompt: string) {
  const traceId = randomUUID();
  const started = performance.now();

  const res = await client.chat.completions.create({ model: 'gpt-4o-mini', messages: [{ role: 'user', content: prompt }] });
  const latency = Math.round(performance.now() - started);
  const output = res.choices[0]?.message?.content ?? '';

  const entry: LogEntry = {
    ts: new Date().toISOString(),
    trace_id: traceId,
    model: 'gpt-4o-mini',
    prompt,
    tools: [],
    latency_ms: latency,
    output
  };

  stream.write(JSON.stringify(entry) + '\\n');
  return { output, traceId };
}
`}
</code></pre>
    <p>
      Rotate files daily (`logs/requests-YYYY-MM-DD.jsonl`), sanitize secrets before writing, and add a
      `--dry-run` flag for offline testing.
    </p>
  </section>

  <section class="card" aria-labelledby="how-title">
    <div class="section-title">
      <div>
        <p class="pill inline">Runbook</p>
        <h2 id="how-title">How to try</h2>
      </div>
      <small>Fast POC checklist</small>
    </div>
    <ol>
      <li>Create `logs/` and add it to `.gitignore`.</li>
      <li>Drop the logger helper above into `scripts/logger.ts`.</li>
      <li>Call `askLLM("Summarize the current module plan")` from a CLI script.</li>
      <li>Inspect `logs/requests.jsonl` to diff prompts across runs.</li>
    </ol>
  </section>

  <section class="card" aria-labelledby="next-title">
    <div class="section-title">
      <div>
        <p class="pill inline">Next</p>
        <h2 id="next-title">Next steps</h2>
      </div>
      <small>Graduating the POC</small>
    </div>
    <ul>
      <li>Enrich with token counts and cost estimation per request.</li>
      <li>Add structured tool call logging (`tool`, `arguments`, `result`).</li>
      <li>Expose a `tail -f`-style viewer to spot regressions quickly.</li>
    </ul>
  </section>
</BaseLayout>
